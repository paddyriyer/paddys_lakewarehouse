"""
SAP ECC Extraction Pipeline (AWS Glue Job)
============================================
Generated by Claude ETL Agent.
Connects to SAP via PyRFC, extracts customer master (KNA1),
and writes to S3 Bronze layer as Delta Lake.

AWS Glue Job Configuration:
  - Worker type: G.1X
  - Number of workers: 4
  - Glue version: 4.0
  - Additional python modules: pyrfc, delta-spark
"""

import boto3
import json
import pyrfc
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, current_timestamp, sha2, concat_ws
from delta.tables import DeltaTable


def get_sap_credentials():
    """Retrieve SAP credentials from AWS Secrets Manager."""
    secrets = boto3.client('secretsmanager')
    response = secrets.get_secret_value(SecretId='mdm/sap/ecc-production')
    return json.loads(response['SecretString'])


def extract_sap_table(conn, table_name: str, fields: list) -> list:
    """Extract table via RFC_READ_TABLE."""
    result = conn.call(
        'RFC_READ_TABLE',
        QUERY_TABLE=table_name,
        FIELDS=fields,
        ROWCOUNT=0  # All rows
    )
    # Parse RFC flat-string result into list of dicts
    field_names = [f['FIELDNAME'] for f in result['FIELDS']]
    rows = [
        dict(zip(field_names, [v.strip() for v in row['WA'].split('|')]))
        for row in result['DATA']
    ]
    return rows


def main():
    # ── Get credentials ──
    sap_creds = get_sap_credentials()

    # ── Connect to SAP ──
    conn = pyrfc.Connection(
        ashost=sap_creds['host'],
        sysnr=sap_creds['sysnr'],
        client=sap_creds['client'],
        user=sap_creds['user'],
        passwd=sap_creds['pass'],
    )

    # ── Extract KNA1 (Customer Master) ──
    kna1_fields = ['KUNNR', 'NAME1', 'NAME2', 'LAND1', 'ORT01',
                   'PSTLZ', 'STRAS', 'TELF1', 'SMTP_ADDR', 'KTOKD']
    rows = extract_sap_table(conn, 'KNA1', kna1_fields)

    # ── Create Spark DataFrame ──
    spark = SparkSession.builder.appName('sap-kna1-extraction').getOrCreate()
    df = spark.createDataFrame(rows)

    # ── Add Bronze metadata columns ──
    df = (df
          .withColumn('_ingestion_ts', current_timestamp())
          .withColumn('_source_system', lit('SAP_ECC'))
          .withColumn('_batch_id', lit('batch_20260207_001'))
          .withColumn('_row_hash', sha2(concat_ws('|', *df.columns), 256)))

    # ── Write to S3 Bronze as Delta Lake ──
    df.write \
        .format('delta') \
        .mode('append') \
        .partitionBy('LAND1') \
        .save('s3://company-mdm-lakehouse-prod/bronze/sap/kna1/')

    print(f'Extracted {df.count()} rows from SAP KNA1 to Bronze')
    conn.close()


if __name__ == '__main__':
    main()
